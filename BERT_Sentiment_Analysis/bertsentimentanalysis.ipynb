{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9174816,"sourceType":"datasetVersion","datasetId":5544794}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\nfrom transformers import BertForSequenceClassification, AdamW\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tqdm import tqdm\nimport torch\nimport warnings\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T06:43:43.450746Z","iopub.execute_input":"2024-08-26T06:43:43.451656Z","iopub.status.idle":"2024-08-26T06:43:43.457539Z","shell.execute_reply.started":"2024-08-26T06:43:43.451599Z","shell.execute_reply":"2024-08-26T06:43:43.456435Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\n# 1. Loading the Dataset\ndata = pd.read_csv('/kaggle/input/sonnnnnn/vodafone_data.csv', delimiter=';')","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:02:29.215915Z","iopub.execute_input":"2024-08-26T06:02:29.216786Z","iopub.status.idle":"2024-08-26T06:02:29.249058Z","shell.execute_reply.started":"2024-08-26T06:02:29.216746Z","shell.execute_reply":"2024-08-26T06:02:29.248217Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Displaying the first few rows\nprint(\"Original Data:\")\nprint(data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:09:21.386372Z","iopub.execute_input":"2024-08-26T06:09:21.386812Z","iopub.status.idle":"2024-08-26T06:09:21.394376Z","shell.execute_reply.started":"2024-08-26T06:09:21.386773Z","shell.execute_reply":"2024-08-26T06:09:21.393254Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Original Data:\n                                         Explanation  Target\n0  Vodafone'a ıyy diyen hayatında hiç vodafone ku...       1\n1  Her yerde çekiyor diye geçtiğim güne lanet ols...       0\n2  Vodafone benim için bitti artık her yerde çeki...       0\n3  Vodafone Sizin de içine mi sızdılar yoksa bili...       0\n4  Saf olmamak gerekir. Güvenlik açığı demek olay...       0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. Downloading the Turkish Stop Words List\nnltk.download('stopwords')\nstop_words = set(stopwords.words('turkish'))","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:09:26.997094Z","iopub.execute_input":"2024-08-26T06:09:26.997970Z","iopub.status.idle":"2024-08-26T06:09:27.004152Z","shell.execute_reply.started":"2024-08-26T06:09:26.997918Z","shell.execute_reply":"2024-08-26T06:09:27.002977Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3. Cleaning Function\ndef clean_text(text):\n    if isinstance(text, str):  # If the input is a string, clean it\n        # Remove special characters (keep numbers)\n        text = re.sub(r'[^\\w\\s]', '', text)\n        # Convert to lowercase\n        text = text.lower()\n        # Remove stop words\n        text = \" \".join([word for word in text.split() if word not in stop_words])\n    else:\n        text = str(text)  # If the input is not a string, convert it to a string\n    return text\n\n# Clean the text in the 'Explanation' column\ndata['Explanation'] = data['Explanation'].apply(clean_text)\n\n# Display the cleaned data\nprint(\"Cleaned Data:\")\nprint(data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:10:57.234554Z","iopub.execute_input":"2024-08-26T06:10:57.234953Z","iopub.status.idle":"2024-08-26T06:10:57.332564Z","shell.execute_reply.started":"2024-08-26T06:10:57.234919Z","shell.execute_reply":"2024-08-26T06:10:57.331418Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Cleaned Data:\n                                         Explanation  Target\n0  vodafonea ıyy diyen hayatında vodafone kullanm...       1\n1  yerde çekiyor geçtiğim güne lanet olsun çekiyo...       0\n2  vodafone benim bitti artık yerde çekiyor aldık...       0\n3  vodafone sizin içine mi sızdılar yoksa bilinçl...       0\n4  saf olmamak gerekir güvenlik açığı demek olayı...       0\n","output_type":"stream"}]},{"cell_type":"code","source":"# 4. Splitting the Dataset into Training, Validation, and Test Sets\nX = data['Explanation']  # Text data\ny = data['Target']  # Labels (0, 1, 2)\n\n# Splitting into training and test sets (70% training, 30% test)\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Splitting the test set into validation and test sets (15% validation, 15% test)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:11:35.318512Z","iopub.execute_input":"2024-08-26T06:11:35.318929Z","iopub.status.idle":"2024-08-26T06:11:35.333607Z","shell.execute_reply.started":"2024-08-26T06:11:35.318893Z","shell.execute_reply":"2024-08-26T06:11:35.332640Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 5. Formatting for BERT\n# Loading the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenization and formatting function\ndef bert_format(texts, labels, max_len=128):\n    input_ids = []\n    attention_masks = []\n    for text in texts:\n        encoded_dict = tokenizer.encode_plus(\n            text,  # Text to tokenize\n            add_special_tokens=True,  # Adding [CLS] and [SEP]\n            max_length=max_len,  # Maximum length\n            padding='max_length',  # Padding to the max length\n            truncation=True,  # Truncation\n            return_attention_mask=True,  # Creating attention mask\n            return_tensors='pt',  # Returning PyTorch tensors\n        )\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Converting to tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values)\n\n    return input_ids, attention_masks, labels\n\n# Converting the training, validation, and test sets to BERT format\ntrain_inputs, train_masks, train_labels = bert_format(X_train, y_train)\nval_inputs, val_masks, val_labels = bert_format(X_val, y_val)\ntest_inputs, test_masks, test_labels = bert_format(X_test, y_test)\n\n# Creating DataLoaders\nbatch_size = 16\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:12:33.369729Z","iopub.execute_input":"2024-08-26T06:12:33.370714Z","iopub.status.idle":"2024-08-26T06:12:42.501506Z","shell.execute_reply.started":"2024-08-26T06:12:33.370638Z","shell.execute_reply":"2024-08-26T06:12:42.500678Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# 6. Training the Model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\noptimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n\n# Starting training\nepochs = 4\n\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n\n    # Training mode\n    model.train()\n    total_loss = 0\n    total_correct = 0\n\n    for step, batch in enumerate(tqdm(train_dataloader)):\n        batch_inputs, batch_masks, batch_labels = batch\n\n        model.zero_grad()\n\n        outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n        loss = outputs.loss\n        logits = outputs.logits\n        total_loss += loss.item()\n\n        # Get predictions and calculate accuracy\n        predictions = torch.argmax(logits, dim=1)\n        correct = (predictions == batch_labels).sum().item()\n        total_correct += correct\n\n        loss.backward()\n        optimizer.step()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    avg_train_accuracy = total_correct / len(X_train)\n    print(f\"Average Training Loss: {avg_train_loss}\")\n    print(f\"Average Training Accuracy: {avg_train_accuracy}\")\n\n    # Validation mode\n    model.eval()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_correct = 0\n\n    all_preds = []\n    all_labels = []\n\n    for batch in val_dataloader:\n        batch_inputs, batch_masks, batch_labels = batch\n\n        with torch.no_grad():\n            outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n\n        loss = outputs.loss\n        logits = outputs.logits\n\n        total_eval_loss += loss.item()\n\n        # Get predictions and calculate accuracy\n        predictions = torch.argmax(logits, dim=1)\n        correct = (predictions == batch_labels).sum().item()\n        total_eval_correct += correct\n\n        all_preds.extend(predictions.cpu().numpy())\n        all_labels.extend(batch_labels.cpu().numpy())\n\n    avg_val_loss = total_eval_loss / len(val_dataloader)\n    avg_val_accuracy = total_eval_correct / len(X_val)\n    precision = precision_score(all_labels, all_preds, average='weighted')\n    recall = recall_score(all_labels, all_preds, average='weighted')\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n\n    print(f\"Validation Loss: {avg_val_loss}\")\n    print(f\"Validation Accuracy: {avg_val_accuracy}\")\n    print(f\"Validation Precision: {precision}\")\n    print(f\"Validation Recall: {recall}\")\n    print(f\"Validation F1 Score: {f1}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T06:45:43.580839Z","iopub.execute_input":"2024-08-26T06:45:43.581728Z","iopub.status.idle":"2024-08-26T08:06:52.749684Z","shell.execute_reply.started":"2024-08-26T06:45:43.581688Z","shell.execute_reply":"2024-08-26T08:06:52.748503Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 212/212 [19:13<00:00,  5.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.7502650156054856\nAverage Training Accuracy: 0.6614429331756357\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7019997433475826\nValidation Accuracy: 0.6882758620689655\nValidation Precision: 0.5749364491308065\nValidation Recall: 0.6882758620689655\nValidation F1 Score: 0.6111047986089106\nEpoch 2/4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 212/212 [19:06<00:00,  5.41s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.6198162399935272\nAverage Training Accuracy: 0.7409816676522768\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.6901961733465609\nValidation Accuracy: 0.72\nValidation Precision: 0.5969323940180448\nValidation Recall: 0.72\nValidation F1 Score: 0.6523276842063999\nEpoch 3/4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 212/212 [18:55<00:00,  5.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.5071898913889561\nAverage Training Accuracy: 0.7903607332939089\nValidation Loss: 0.6175025416457135\nValidation Accuracy: 0.76\nValidation Precision: 0.7743671285398821\nValidation Recall: 0.76\nValidation F1 Score: 0.7327264037574498\nEpoch 4/4\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 212/212 [18:57<00:00,  5.36s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 0.37615716782451236\nAverage Training Accuracy: 0.8518628030751035\nValidation Loss: 0.6767186690931735\nValidation Accuracy: 0.7655172413793103\nValidation Precision: 0.7590840619764833\nValidation Recall: 0.7655172413793103\nValidation F1 Score: 0.7531716069004789\n","output_type":"stream"}]},{"cell_type":"code","source":"# 7. Modeli Kaydetme\nmodel.save_pretrained('model1')\ntokenizer.save_pretrained('tokenizer1')","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:13:48.604723Z","iopub.execute_input":"2024-08-26T08:13:48.605611Z","iopub.status.idle":"2024-08-26T08:13:49.279788Z","shell.execute_reply.started":"2024-08-26T08:13:48.605557Z","shell.execute_reply":"2024-08-26T08:13:49.278918Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('tokenizer1/tokenizer_config.json',\n 'tokenizer1/special_tokens_map.json',\n 'tokenizer1/vocab.txt',\n 'tokenizer1/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"8.\ndef predict(text, model, tokenizer, max_len=128):\n    encoded_dict = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_len,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    input_ids = encoded_dict['input_ids']\n    attention_mask = encoded_dict['attention_mask']\n\n    model.eval()\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        predicted_class = torch.argmax(logits, dim=1).item()\n\n    return predicted_class\n\n# Kullanıcıdan giriş alma ve tahmin yapma\nuser_input = input(\"Tahmin yapmak için bir metin girin: \")\nprediction = predict(user_input, model, tokenizer)\nprint(f\"Tahmin edilen sınıf: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T08:16:16.455256Z","iopub.execute_input":"2024-08-26T08:16:16.455663Z","iopub.status.idle":"2024-08-26T08:16:26.285023Z","shell.execute_reply.started":"2024-08-26T08:16:16.455629Z","shell.execute_reply":"2024-08-26T08:16:26.283926Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdin","text":"Tahmin yapmak için bir metin girin:  vodafone berbat\n"},{"name":"stdout","text":"Tahmin edilen sınıf: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}