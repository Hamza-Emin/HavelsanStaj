{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7My4kYCOUumF"
      },
      "outputs": [],
      "source": [
        "!pip install https://huggingface.co/turkish-nlp-suite/tr_core_news_trf/resolve/main/tr_core_news_trf-1.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "nlp = spacy.load('tr_core_news_trf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsO34NTLVxe_",
        "outputId": "66311160-d390-45e4-d085-cddd499b4f97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy_transformers/layers/hf_shim.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Vodafone Kick kullanırken Youtube çok kasıyordu ama Turkcell kullanırken sıkıntı yoktu.\"\n",
        "entities = [\"Vodafone\", \"Kick\", \"Youtube\", \"Turkcell\"]"
      ],
      "metadata": {
        "id": "_xXY5uFHVkDI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ab_model(entity, text):\n",
        "  pass"
      ],
      "metadata": {
        "id": "qAKYvLQ9Uwqi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_sentences_with_conjunction(entities1, text):\n",
        "\n",
        "    doc = nlp(text) # metnin tokenize haline getirilmesi\n",
        "    split_sentences = [] # çıkarılan cümlelerin tutulacağı dizinin başlatılması\n",
        "    add_old_entity = False # ilk durumda eski entity eklenmesi bool değeri False verilir.\n",
        "\n",
        "    for token in doc: # metin içindeki her bir kelime için:\n",
        "      sentence = [] # elde edilen metnin tutulacağ dizinin başlatılması\n",
        "      entities = [token for token in doc if token.text in entities1] # entity lerin metin içinden tanınması\n",
        "      if token.is_sent_end and token in entities: # token en sonda ise ve bir entity ise:\n",
        "        entity = doc[token.i] # entity metin içinden alınır ve bir değişkende tutulur\n",
        "        text = doc[:token.i].text.strip() # sondaki entityden önceki bütün metin bir değişkende tutulur\n",
        "        sentence.append(entity.text + \" \" + text) # başa sondaki entity gelecek şekilde bütün metin entity nin sonunda eklenir\n",
        "        return sentence # son metnin tutulduğu dizi döndürülür\n",
        "\n",
        "    for sent in doc.sents: # metnin her bir cümlesi için:\n",
        "        conjunctions = [token for token in sent if token.pos_ in ['CCONJ', 'SCONJ'] or token.text == \"\\0\"] # bağlaçların metin içinden tanınması\n",
        "        entities = [token for token in sent if token.text in entities1] # entity lerin metin içinden tanınması\n",
        "        k = 0 # ilk bağlaç için bir sayaç oluşturulması\n",
        "        if conjunctions: # metinde bağlaç var ise:\n",
        "          for conjunction in conjunctions: # her bir bağlaç için:\n",
        "            if k==0: # ilk bağlaç için:\n",
        "              if sent[conjunction.i-1] in entities and sent[conjunction.i+1] in entities: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity ise:\n",
        "                old_entity = sent[conjunction.i-1] # önceki entity bir değişkende tutulur\n",
        "                add_old_entity = True # eski entity sonradan ekleneceği için bool değişkeni True yapılır\n",
        "              else: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity değil ise:\n",
        "                text = sent[:conjunction.i].text.strip() # bağlaçtan önceki cümle metinden alınır\n",
        "                split_sentences.append(text) # alınan metin diziye eklenir\n",
        "              k += 1 # ilk bağlaçtan çıkıldığı için sayaç arttırılır\n",
        "              old_conjunction = conjunction # eski bağlaç bir değişkende tutulur\n",
        "            else: # ilk bağlaçtan sonraki bağlaçlar ise:\n",
        "              if sent[conjunction.i-1] in entities and sent[conjunction.i+1] in entities: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity ise:\n",
        "                old_entity = sent[conjunction.i-1] # önceki entity bir değişkende tutulur\n",
        "                add_old_entity = True # eski entity sonradan ekleneceği için bool değişkeni True yapılır\n",
        "              else: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity değil ise:\n",
        "                text = sent[old_conjunction.i+1:conjunction.i].text.strip() # eski bağlaçtan şu anki bağlaça kadar olan cümle metinden alınır\n",
        "                split_sentences.append(text) # alınan metin diziye eklenir\n",
        "                if add_old_entity == True: # eski entity nin eklenmesi gerekiyor ise:\n",
        "                    text = sent[old_entity.i].text.strip() # eski entity cümleden alınır\n",
        "                    text2 = sent[old_conjunction.i+2:conjunction.i].text.strip() # eski bağlaçtan ve entityden sonra şu anki bağlaça kadar olan cümle metinden alınır\n",
        "                    split_sentences.append(text + \" \" + text2) # eski entity ile alınan cümle arasına bir boşluk eklenerek diziye eklenir\n",
        "                    add_old_entity = False # eski entity alındığı için bool False değerine getirilir\n",
        "              old_conjunction = conjunction # eski bağlaç bir değişkende tutulur\n",
        "              k += 1 # baglac sayacı bir arttırılır\n",
        "          split_sentences.append(sent[conjunction.i + 1:].text.strip()) # son baglactan sonra kalan metin alınır\n",
        "\n",
        "    while(\"\" in split_sentences): # son alınan metin boş string ise diziden silinir\n",
        "      split_sentences.remove(\"\")\n",
        "\n",
        "    return split_sentences"
      ],
      "metadata": {
        "id": "XUuCZvSLVlyU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaç tane entity olduğunu sayar\n",
        "def howmanyentity(sentence, entity):\n",
        "    count = 0\n",
        "\n",
        "    words = sentence.split(\" \")\n",
        "    for i in words:\n",
        "        if i in entity:\n",
        "            count = count+1\n",
        "\n",
        "    return count"
      ],
      "metadata": {
        "id": "c557-oKcVyeP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fiilimsi_suffixes = [\n",
        "    \"ken\", \"alı\", \"eli\", \"madan\", \"meden\", \"ince\", \"ınca\", \"unca\", \"ünce\",\n",
        "    \"ıp\", \"up\", \"üp\", \"arak\", \"erek\", \"dıkça\", \"dikçe\", \"dukça\", \"dükçe\",\n",
        "    \"tıkça\", \"tikçe\", \"tukça\", \"tükçe\", \"a\", \"e\", \"r\", \"maz\", \"mez\",\n",
        "    \"casına\", \"cesine\", \"meksizin\", \"maksızın\", \"dığında\", \"diğinde\",\n",
        "    \"duğunda\", \"düğünde\", \"tığında\", \"tiğinde\", \"tuğunda\", \"tüğünde\"\n",
        "]\n",
        "\n",
        "def find_fiilimsi(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    fiilimsis = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Check if the token ends with any of the fiilimsi suffixes\n",
        "        if any(token.text.endswith(suffix) for suffix in fiilimsi_suffixes):\n",
        "            # Check dependency and POS tags to filter fiilimsi\n",
        "            if token.dep_ in [\"advcl\", \"ccomp\", \"amod\", \"acl\"] or (token.pos_ in ['VERB', 'AUX'] and token.dep_ not in [\"ROOT\"]):\n",
        "                fiilimsis.append(token.text)\n",
        "\n",
        "    return fiilimsis"
      ],
      "metadata": {
        "id": "97RpDkUaVqS2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitting(entity : list, sentence: str):\n",
        "\n",
        "    sentenceresults = list()\n",
        "\n",
        "    if sentence.endswith(\".\"):\n",
        "        sentence = sentence[0:-1]\n",
        "\n",
        "    numberofentities=howmanyentity(sentence=sentence,entity=entity) #change this function to handle empty spaces in the end\n",
        "    print(f\"numberofentities of this function is : {numberofentities}\")\n",
        "    if ( numberofentities == 1): # if there is only 1 entity\n",
        "        sentenceresults.append(sentence)\n",
        "\n",
        "    elif(numberofentities >1 ):\n",
        "        numberoffiilimsi = len(find_fiilimsi(sentence=sentence))\n",
        "        print(f\"Number of fiilimsi is :{numberoffiilimsi}\")\n",
        "\n",
        "        if(numberoffiilimsi==1) :\n",
        "            print(\"Hamza\")\n",
        "            entitiypositions = []\n",
        "            for i in entity:\n",
        "                entitiypositions.append(sentence.index(i))\n",
        "\n",
        "            fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0])\n",
        "            print(\"Emin\")\n",
        "            whereisfiilimsi = all(index < fiilimsiposition for index in entitiypositions)\n",
        "            if whereisfiilimsi:\n",
        "\n",
        "\n",
        "                for i in entitiypositions:\n",
        "                    entitylen = len(sentence[i:].split()[0])\n",
        "                    print(f\" Empty len is {entitylen}\")\n",
        "                    print(f\"entitiyposition: {entitylen}, fiilimsipositions: {fiilimsiposition}\")\n",
        "                    if i == 0:\n",
        "                        temp_sentence = sentence[:entitylen] + \" \" + sentence[fiilimsiposition:]\n",
        "                    if i !=0:\n",
        "                        temp_sentence = sentence[i:i+entitylen]+ \" \" + sentence[fiilimsiposition:]\n",
        "                    sentenceresults.append(temp_sentence)\n",
        "\n",
        "\n",
        "            else : # that means there is a fiilimsi between entities. there can be entity fiilimsi entity , entity entity fiilimsi entity\n",
        "                beforeentitiesposition = [] # BEFORE THE FİİLİMSİ ENTİTİES\n",
        "                afterentitiesposition = [] # AFTER THE FİİLİMSİ ENTİTİES\n",
        "                fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0]) # FİİLİMSİ POİZİTON\n",
        "\n",
        "                for i in entity:\n",
        "                    if sentence.find(i) < fiilimsiposition:\n",
        "                        beforeentitiesposition.append(sentence.find(i))\n",
        "                    elif sentence.find(i) > fiilimsiposition:\n",
        "                        afterentitiesposition.append(sentence.find(i))\n",
        "\n",
        "                #handle before entities\n",
        "                #handle after entities\n",
        "                #append the result\n",
        "                for positions in beforeentitiesposition: # [\"Twitch kick güzelken whatsapp çalışmıyor\"] [\"Twitch: 0 kick : 7 whatsapp :20\"][\"güzelken=12\"]\n",
        "                    print(\"--------------------\")\n",
        "\n",
        "                    entitylen = len(sentence[fiilimsiposition:].split()[0]) # güzelken = 12 güzelken len = 8\n",
        "                    subsentence = sentence[:fiilimsiposition+entitylen]   # sentence[:20] whatsapp dan öncesi\n",
        "                    tempb = subsentence\n",
        "                    for words in entity: #sentence is Twitch kick güzelken beforeentitiesposition is [twitch pozisyonu kick pozisyonu]\n",
        "                        if subsentence.find(words) == positions and len(beforeentitiesposition)!=1 : #  0 ve 7 eğer 20 eğer eşit değilse  0 7 20 yani 20 alıcak\n",
        "                            tempb=tempb.replace(words,\"\") # entity list contains Twitch kick turkiye.\n",
        "                    sentenceresults.append(tempb)\n",
        "\n",
        "\n",
        "\n",
        "                for positions in afterentitiesposition:\n",
        "                    print(\"////////////////////\")\n",
        "\n",
        "                    entitylen = len(sentence[fiilimsiposition:].split()[0])\n",
        "                    subsentence = sentence[fiilimsiposition+entitylen:]\n",
        "                    tempa = subsentence\n",
        "                    for words in entity: # küçük bir hata var\n",
        "                        if subsentence.find(words) == positions : # with last part after the and  if the entity  is less than the position\n",
        "                            tempa = tempa.replace(words,\"\")\n",
        "                    sentenceresults.append(tempa)\n",
        "\n",
        "\n",
        "\n",
        "    return sentenceresults"
      ],
      "metadata": {
        "id": "nx8x4exrVrcR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_entities(entities,sentence):\n",
        "    entit = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word in entities:\n",
        "            entit.append(word)\n",
        "    return entit\n",
        "\n",
        "sentence1 =\"Vodafone RedTarife güzelken çok pahalılar\"\n",
        "entity = [\"Vodafone\",\"Turkcell\",\"RedTarife\"]\n",
        "find_entities(entity,sentence1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYB6Qq1dWT1C",
        "outputId": "9df41f68-c642-431b-e5ea-1d2e5f093048"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vodafone', 'RedTarife']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Dependency_Parser(entities:list, text:str):\n",
        "    result = get_split_sentences_with_conjunction(entities, text)\n",
        "    sentence_results = []\n",
        "\n",
        "    for sentence in result:\n",
        "        entity_list = find_entities(entities,sentence)\n",
        "        sentence_results.append(splitting(entity_list,sentence))\n",
        "    return sentence_results"
      ],
      "metadata": {
        "id": "XDSXod-7VrLF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_result = []\n",
        "sentence_result = Dependency_Parser(entities, sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHe7tUE5WBTK",
        "outputId": "18b58a8d-322c-4bd4-9d2e-d1742eef4208"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numberofentities of this function is : 3\n",
            "Number of fiilimsi is :1\n",
            "Hamza\n",
            "Emin\n",
            "--------------------\n",
            "--------------------\n",
            "////////////////////\n",
            "numberofentities of this function is : 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[' Kick kullanırken', 'Vodafone  kullanırken', ' Youtube çok kasıyordu'],\n",
              " ['Turkcell kullanırken sıkıntı yoktu']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODELI YUKLEME\n",
        "# model path-to-save isimli klasör içine koyulmalı\n",
        "# Load tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(path +'/Tokenizer')\n",
        "\n",
        "# Load model\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(path +'/Model')"
      ],
      "metadata": {
        "id": "vRhdY06VWVJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KULLANICI GIRISI ILE TAHMIN FONKSIYONU\n",
        "\n",
        "def Get_sentiment(Review, Tokenizer=bert_tokenizer, Model=bert_model):\n",
        "\t# Convert Review to a list if it's not already a list\n",
        "\tif not isinstance(Review, list):\n",
        "\t\tReview = [Review]\n",
        "\n",
        "\tInput_ids, Token_type_ids, Attention_mask = Tokenizer.batch_encode_plus(Review,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding=True,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ttruncation=True,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tmax_length=1024,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn_tensors='tf').values()\n",
        "\tprediction = Model.predict([Input_ids, Token_type_ids, Attention_mask])\n",
        "\n",
        "\t# Use argmax along the appropriate axis to get the predicted labels\n",
        "\tpred_labels = tf.argmax(prediction.logits, axis = 1)\n",
        "\n",
        "\t# Convert the TensorFlow tensor to a NumPy array and then to a list to get the predicted sentiment labels\n",
        "\tpred_labels = [label[i] for i in pred_labels.numpy().tolist()]\n",
        "\treturn pred_labels\n"
      ],
      "metadata": {
        "id": "-iQ3gwmivobF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentence_result:\n",
        "  print(Get_sentiment(sentence))"
      ],
      "metadata": {
        "id": "SZBM0OajvtsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}