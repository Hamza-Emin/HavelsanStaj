{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "entities1 = [\"vodafon\",\"türkcell\", \"kick\",\"twitch\"] \n",
    "\n",
    "sentence1 = \"vodafon güzel. türkcell kötü. kick iyi. twitch mükkemmel.\"\n",
    "\n",
    "def find_sentence_counter(sentencetext, entities):\n",
    "    countermap = dict()\n",
    "    if sentencetext[-1] == \".\": #remove the last dot(.) to get the correct sentence length\n",
    "        lt = len(sentencetext)\n",
    "        sentencetext = sentencetext[0:lt-1]\n",
    "    sentencelist = sentencetext.split(\".\") # get the sentence list\n",
    "    index = 0\n",
    "    count = 0\n",
    "    for sentence in sentencelist: # for each sentence in sentence list, calculate the aspect number in the sentence\n",
    "        sentence_words = sentence.split(\" \") # get the words\n",
    "        \n",
    "        for words in sentence_words:\n",
    "            if words in entities:\n",
    "                count = count +1\n",
    "                countermap[index] = count\n",
    "            \n",
    "        index= index +1\n",
    "        count=0 \n",
    "    return countermap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mydict=find_sentence_counter(sentence1,entities1)\n",
    "\n",
    "\n",
    "result = False\n",
    "\n",
    "\n",
    "if all(mydict.values()) == 1:\n",
    "    result=True\n",
    "\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HAMZA\\AppData\\Roaming\\Python\\Python310\\site-packages\\spacy_transformers\\layers\\hf_shim.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "c:\\Python310\\lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kullanmadan']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('tr_core_news_trf')\n",
    "\n",
    "\n",
    "fiilimsi_suffixes = [\n",
    "    \"ken\", \"alı\", \"eli\", \"madan\", \"meden\", \"ince\", \"ınca\", \"unca\", \"ünce\",\n",
    "    \"ıp\", \"up\", \"üp\", \"arak\", \"erek\", \"dıkça\", \"dikçe\", \"dukça\", \"dükçe\",\n",
    "    \"tıkça\", \"tikçe\", \"tukça\", \"tükçe\", \"a\", \"e\", \"r\", \"maz\", \"mez\", \n",
    "    \"casına\", \"cesine\", \"meksizin\", \"maksızın\", \"dığında\", \"diğinde\", \n",
    "    \"duğunda\", \"düğünde\", \"tığında\", \"tiğinde\", \"tuğunda\", \"tüğünde\"\n",
    "]\n",
    "\n",
    "def find_fiilimsi(sentence):\n",
    "    lengthofsentence = len(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    fiilimsis = []\n",
    "    noun_positions = []\n",
    "    \n",
    "    for token in doc:\n",
    "    \n",
    "        if any(token.text.endswith(suffix) for suffix in fiilimsi_suffixes):\n",
    "            \n",
    "            if token.idx != len(sentence) - len(token.text) :\n",
    "                head_token = token.head\n",
    "               # print(f\" {token.dep_}\")\n",
    "                if token.dep_ == \"advcl\" and (head_token.pos_ == 'VERB' or head_token.pos_ =='ADJ') :\n",
    "                    \n",
    "                 # 84 -9 = 75\n",
    "                    fiilimsis.append(token.text)\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "        elif token.pos_ == \"NOUN\":\n",
    "            noun_positions.append((token.text, token.idx))\n",
    "    \n",
    "    \n",
    "\n",
    "    return fiilimsis\n",
    "\n",
    "sentence = \"Vodafone kullanmadan yoruldum.\"\n",
    "res=find_fiilimsi(sentence)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities1 = [\"Vodafone\",\"Turkcell\"]\n",
    "\n",
    "sentence1 = \"Vodafone Turkcell güzelken çok pahalılar\"\n",
    "example5 = \"Twitch kick güzelken whatsapp çalışmıyor\" # \n",
    "ent5 = [\"Twitch\",\"kick\",\"whatsapp\"]\n",
    "sentence2 = \"\"\n",
    "def howmanyentity(sentence, entity):\n",
    "    count = 0\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    for i in words:\n",
    "        if i in entity:\n",
    "            count= count+1\n",
    "            \n",
    "\n",
    "    return count\n",
    "\n",
    "howmanyentity(example5,ent5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Vodafone memnun değilim'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def usecases(mydict,sentencetext,entities): # mydict contains dict of  sentences id  and their entity values.\n",
    "   \n",
    "    print(len(entities))\n",
    "    print(len(find_fiilimsi(sentencetext)))\n",
    "    if(len(entities)==1 and (\".\" not in sentencetext) and (len(find_fiilimsi(sentencetext)) ==1  or (len(find_fiilimsi(sentencetext))) == 0)):\n",
    "        return sentencetext ## assuming there is 1 entity and 1 sentence\n",
    "\n",
    "\n",
    "    if len(entities) > len(sentencetext.split(\".\")):\n",
    "        \n",
    "        resultofcomplex = []\n",
    "        sentences = sentencetext.split(\".\")\n",
    "        # check each sentence for entity number\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if howmanyentity(sentence,entities) > 1:\n",
    "                # that means in this sentence there is more than 1 entitiy so we should make 2 sentences(if we assume sentence contains 1 fiilimsi and 2 sentences)\n",
    "                # find the fiilimsi and their number if it is 1 make it two sentences\n",
    "                # while doing it check the entitiy places if two of the entity places before the fiilimsi entity fiilimsi rest of the sentence\n",
    "                # if the fiilimsi is between entities, split it right on the fiilimsi \n",
    "                print(sentence)\n",
    "\n",
    "                myentities = []\n",
    "                for i in entities:\n",
    "                    if i in sentence:\n",
    "                        myentities.append(i)\n",
    "                \n",
    "                fiilimsiler = find_fiilimsi(sentence) # lets consider there is one 2 entity and one fiilimsi\n",
    "                \n",
    "                \n",
    "                positionfiilimsi = sentence.find(fiilimsiler[0])\n",
    "                positionentity1 = sentence.find(myentities[0]) # find corrresponding entities and put them in here\n",
    "                positionentity2 = sentence.find(myentities[1])\n",
    "                \n",
    "                \n",
    "\n",
    "                if(positionentity1 < positionfiilimsi  and positionentity2 < positionfiilimsi):\n",
    "                    # 2 entitiy de fiilimsiden önce\n",
    "                    if(positionentity1<positionentity2):\n",
    "                        \n",
    "                        \n",
    "                        if positionentity1 ==0: \n",
    "                            sentence1  = sentence[positionentity1:positionentity2] + sentence[positionfiilimsi:]\n",
    "                            sentence2 = sentence[positionentity2:positionfiilimsi] + sentence[positionfiilimsi:]\n",
    "                            resultofcomplex.append(sentence1)\n",
    "                            resultofcomplex.append(sentence2)\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            sentence1  = sentence[0:positionentity1] +sentence[positionentity1]+ sentence[positionfiilimsi:]\n",
    "                            sentence2 = sentence[positionentity2:positionfiilimsi] + sentence[positionfiilimsi:]\n",
    "                            resultofcomplex.append(sentence1)\n",
    "                            resultofcomplex.append(sentence2)\n",
    "                    else:\n",
    "                        if positionentity2 == 0 :\n",
    "\n",
    "                            sentence1 = sentence[positionentity2] + sentence[positionfiilimsi]\n",
    "                            sentence2 = sentence[positionentity1:positionfiilimsi] + sentence[positionfiilimsi]\n",
    "                            resultofcomplex.append(sentence1)\n",
    "                            resultofcomplex.append(sentence2)\n",
    "                        else :\n",
    "                            sentence1 = sentence[0:positionentity2]+sentence[positionentity2] + sentence[positionfiilimsi]\n",
    "                            sentence2 = sentence[positionentity1:positionfiilimsi] + sentence[positionfiilimsi]\n",
    "                            resultofcomplex.append(sentence1)\n",
    "                            resultofcomplex.append(sentence2)\n",
    "    \n",
    "                elif ( positionentity1 < positionfiilimsi and positionentity2 > positionfiilimsi):\n",
    "                    \n",
    "                    sentence1 = sentence[positionentity1:(positionfiilimsi + len(fiilimsiler[0]))]\n",
    "                    sentence2 = sentence[(positionfiilimsi + len(fiilimsiler[0])):]\n",
    "                    resultofcomplex.append(sentence1)\n",
    "                    resultofcomplex.append(sentence2)\n",
    "            \n",
    "                elif(positionentity2 < positionfiilimsi and positionfiilimsi < positionentity1):\n",
    "                \n",
    "                    sentence1 = sentence[positionentity2:(positionfiilimsi + len(fiilimsiler[0]))]\n",
    "                    sentence2 = sentence[(positionfiilimsi + len(fiilimsiler[0])):]\n",
    "                    resultofcomplex.append(sentence1)\n",
    "                    resultofcomplex.appen(sentence2)\n",
    "                \n",
    "                return resultofcomplex\n",
    "                \n",
    "            # there is only one entity in whole sentence so we simply return it as it is.\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "        # bağlaç var fiilimsi yok\n",
    "        # fiilimsi var bağlaç yok\n",
    "        # bağlaç ve use case var\n",
    "        # sonunda vodafone var\n",
    "# 3 aspect ile olanı yap ve bitir.\n",
    "example = \"Vodafone güzelken Turkcell çok yavaş kalması can sıkıcı\"\n",
    "example2 = \"Vodafone kick izlerken donmalar oluyor\"\n",
    "example3 = \"Vodafone kullanmadan yoruldum\"\n",
    "example4 = \"Vodafone memnun değilim\"\n",
    "entities = [\"Vodafone\"]\n",
    "dictinoray = find_sentence_counter(example,entities)\n",
    "result = usecases(sentencetext=example4,entities=[\"Vodafone\"],mydict=dictinoray)\n",
    "result\n",
    "        \n",
    "            \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      "--------------------\n",
      "--------------------\n",
      "////////////////////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' kick güzelken', 'Twitch  güzelken', ' whatsapp çalışmıyor']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitting(entity : list, sentence: str):\n",
    "\n",
    "    sentenceresults = list()\n",
    "\n",
    "    if sentence.endswith(\".\"):\n",
    "        sentence = sentence[0:-1]\n",
    "\n",
    "    \n",
    "    numberofentities=howmanyentity(sentence=sentence,entity=entity) #change this function to handle empty spaces in the end\n",
    "    print(f\"numberofentities of this function is : {numberofentities}\") \n",
    "    if ( numberofentities == 1): # if there is only 1 entity\n",
    "        sentenceresults.append(sentence)\n",
    "\n",
    "    elif(numberofentities >1 ):\n",
    "        numberoffiilimsi = len(find_fiilimsi(sentence=sentence))\n",
    "        print(f\"Number of fiilimsi is :{numberoffiilimsi}\")\n",
    "\n",
    "        if(numberoffiilimsi==1) :\n",
    "            print(\"Hamza\") \n",
    "            entitiypositions = []\n",
    "            for i in entity:\n",
    "                entitiypositions.append(sentence.index(i))\n",
    "\n",
    "            fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0])\n",
    "            print(\"Emin\")\n",
    "            whereisfiilimsi = all(index < fiilimsiposition for index in entitiypositions)\n",
    "            if whereisfiilimsi:\n",
    "            \n",
    "                \n",
    "                for i in entitiypositions:\n",
    "                    entitylen = len(sentence[i:].split()[0])\n",
    "                    print(f\" Empty len is {entitylen}\")\n",
    "                    print(f\"entitiyposition: {entitylen}, fiilimsipositions: {fiilimsiposition}\")\n",
    "                    if i == 0:\n",
    "                        temp_sentence = sentence[:entitylen] + \" \" + sentence[fiilimsiposition:]\n",
    "                    if i !=0:\n",
    "                        temp_sentence = sentence[i:i+entitylen]+ \" \" + sentence[fiilimsiposition:]\n",
    "                    sentenceresults.append(temp_sentence)\n",
    "                    \n",
    "\n",
    "            else : # that means there is a fiilimsi between entities. there can be entity fiilimsi entity , entity entity fiilimsi entity\n",
    "                beforeentitiesposition = [] # BEFORE THE FİİLİMSİ ENTİTİES\n",
    "                afterentitiesposition = [] # AFTER THE FİİLİMSİ ENTİTİES\n",
    "                fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0]) # FİİLİMSİ POİZİTON \n",
    "\n",
    "                for i in entity:\n",
    "                    if sentence.find(i) < fiilimsiposition:\n",
    "                        beforeentitiesposition.append(sentence.find(i))\n",
    "                    elif sentence.find(i) > fiilimsiposition:\n",
    "                        afterentitiesposition.append(sentence.find(i))\n",
    "                \n",
    "                #handle before entities\n",
    "                #handle after entities\n",
    "                #append the result\n",
    "                for positions in beforeentitiesposition: # [\"Twitch kick güzelken whatsapp çalışmıyor\"] [\"Twitch: 0 kick : 7 whatsapp :20\"][\"güzelken=12\"]\n",
    "                    print(\"--------------------\")\n",
    "                    \n",
    "                    entitylen = len(sentence[fiilimsiposition:].split()[0]) # güzelken = 12 güzelken len = 8\n",
    "                    subsentence = sentence[:fiilimsiposition+entitylen]   # sentence[:20] whatsapp dan öncesi\n",
    "                    tempb = subsentence\n",
    "                    for words in entity: #sentence is Twitch kick güzelken beforeentitiesposition is [twitch pozisyonu kick pozisyonu]\n",
    "                        if subsentence.find(words) == positions and len(beforeentitiesposition)!=1 : #  0 ve 7 eğer 20 eğer eşit değilse  0 7 20 yani 20 alıcak\n",
    "                            tempb=tempb.replace(words,\"\") # entity list contains Twitch kick turkiye. \n",
    "                    sentenceresults.append(tempb)\n",
    "                    \n",
    "                \n",
    "\n",
    "                for positions in afterentitiesposition: \n",
    "                    print(\"////////////////////\")\n",
    "                   \n",
    "                    entitylen = len(sentence[fiilimsiposition:].split()[0])\n",
    "                    subsentence = sentence[fiilimsiposition+entitylen:]\n",
    "                    tempa = subsentence\n",
    "                    for words in entity: # küçük bir hata var\n",
    "                        if subsentence.find(words) == positions : # with last part after the and  if the entity  is less than the position\n",
    "                            tempa = tempa.replace(words,\"\")\n",
    "                    sentenceresults.append(tempa)\n",
    "                    \n",
    "        \n",
    "\n",
    "    return sentenceresults\n",
    "\n",
    "#splitting([\"Vodafone\",\"Turkcell\"],\"Vodafone Turkcell güzelken çok pahalılar\") # that works\n",
    "entities3= [\"Vodafone\",\"Turkcell\"]\n",
    "example3 = \"Vodafone Turkcell güzelken çok pahalılar\"  # correct  # \n",
    "example16 = \"Vodafone güzelken Turkcell çok yavaş kalması can sıkıcı\"#not correct # correct -if ı change beforepostion from > to <this one comes uncorrect\n",
    "example5 = \"Twitch kick güzelken whatsapp çalışmıyor\" # correct #  correct if ı change beforeposition from > to < this one comes uncorrect\n",
    "ent5 = [\"Twitch\",\"kick\",\"whatsapp\"]\n",
    "\n",
    "splitting(entity=ent5,sentence= example5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 2\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      " Empty len is 8\n",
      "entitiyposition: 8, fiilimsipositions: 18\n",
      " Empty len is 8\n",
      "entitiyposition: 8, fiilimsipositions: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Vodafone güzelken çok pahalılar', 'Turkcell güzelken çok pahalılar']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitting(entity=entities3,sentence= example3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 2\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      "--------------------\n",
      "////////////////////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Vodafone güzelken', ' Turkcell çok yavaş kalması can sıkıcı']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitting(entity=entities3,sentence= example16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 3\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      "--------------------\n",
      "--------------------\n",
      "////////////////////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' kick güzelken', 'Twitch  güzelken', ' whatsapp çalışmıyor']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitting(entity=ent5,sentence= example5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
