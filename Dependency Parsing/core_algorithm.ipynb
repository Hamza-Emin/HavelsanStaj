{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('tr_core_news_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howmanyentity(sentence, entity):\n",
    "    count = 0\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    for i in words:\n",
    "        if i in entity:\n",
    "            count= count+1\n",
    "            \n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiilimsi_suffixes = [\n",
    "    \"ken\", \"alı\", \"eli\", \"madan\", \"meden\", \"ince\", \"ınca\", \"unca\", \"ünce\",\n",
    "    \"ıp\", \"up\", \"üp\", \"arak\", \"erek\", \"dıkça\", \"dikçe\", \"dukça\", \"dükçe\",\n",
    "    \"tıkça\", \"tikçe\", \"tukça\", \"tükçe\", \"a\", \"e\", \"r\", \"maz\", \"mez\", \n",
    "    \"casına\", \"cesine\", \"meksizin\", \"maksızın\", \"dığında\", \"diğinde\", \n",
    "    \"duğunda\", \"düğünde\", \"tığında\", \"tiğinde\", \"tuğunda\", \"tüğünde\"\n",
    "]\n",
    "\n",
    "def find_fiilimsi(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    fiilimsis = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Check if the token ends with any of the fiilimsi suffixes\n",
    "        if any(token.text.endswith(suffix) for suffix in fiilimsi_suffixes):\n",
    "            # Check dependency and POS tags to filter fiilimsi\n",
    "            if token.dep_ in [\"advcl\", \"ccomp\", \"amod\", \"acl\"] or (token.pos_ in ['VERB', 'AUX'] and token.dep_ not in [\"ROOT\"]):\n",
    "                fiilimsis.append(token.text)\n",
    "    \n",
    "    return fiilimsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_sentences_with_conjunction(entities1, text):\n",
    "   \n",
    "    doc = nlp(text) # metnin tokenize haline getirilmesi\n",
    "    split_sentences = [] # çıkarılan cümlelerin tutulacağı dizinin başlatılması\n",
    "    add_old_entity = False # ilk durumda eski entity eklenmesi bool değeri False verilir.\n",
    "\n",
    "    for token in doc: # metin içindeki her bir kelime için:\n",
    "      sentence = [] # elde edilen metnin tutulacağ dizinin başlatılması\n",
    "      entities = [token for token in doc if token.text in entities1] # entity lerin metin içinden tanınması\n",
    "      if token.is_sent_end and token in entities: # token en sonda ise ve bir entity ise:\n",
    "        entity = doc[token.i] # entity metin içinden alınır ve bir değişkende tutulur\n",
    "        text = doc[:token.i].text.strip() # sondaki entityden önceki bütün metin bir değişkende tutulur\n",
    "        sentence.append(entity.text + \" \" + text) # başa sondaki entity gelecek şekilde bütün metin entity nin sonunda eklenir\n",
    "        return sentence # son metnin tutulduğu dizi döndürülür\n",
    "\n",
    "    for sent in doc.sents: # metnin her bir cümlesi için:\n",
    "        conjunctions = [token for token in sent if token.pos_ in ['CCONJ', 'SCONJ'] or token.text == \"\\0\"] # bağlaçların metin içinden tanınması\n",
    "        entities = [token for token in sent if token.text in entities1] # entity lerin metin içinden tanınması\n",
    "        k = 0 # ilk bağlaç için bir sayaç oluşturulması\n",
    "        if conjunctions: # metinde bağlaç var ise:\n",
    "          for conjunction in conjunctions: # her bir bağlaç için:\n",
    "            if k==0: # ilk bağlaç için:\n",
    "              if sent[conjunction.i-1] in entities and sent[conjunction.i+1] in entities: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity ise:\n",
    "                old_entity = sent[conjunction.i-1] # önceki entity bir değişkende tutulur\n",
    "                add_old_entity = True # eski entity sonradan ekleneceği için bool değişkeni True yapılır\n",
    "              else: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity değil ise:\n",
    "                text = sent[:conjunction.i].text.strip() # bağlaçtan önceki cümle metinden alınır\n",
    "                split_sentences.append(text) # alınan metin diziye eklenir\n",
    "              k += 1 # ilk bağlaçtan çıkıldığı için sayaç arttırılır\n",
    "              old_conjunction = conjunction # eski bağlaç bir değişkende tutulur\n",
    "            else: # ilk bağlaçtan sonraki bağlaçlar ise:\n",
    "              if sent[conjunction.i-1] in entities and sent[conjunction.i+1] in entities: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity ise:\n",
    "                old_entity = sent[conjunction.i-1] # önceki entity bir değişkende tutulur\n",
    "                add_old_entity = True # eski entity sonradan ekleneceği için bool değişkeni True yapılır\n",
    "              else: # eğer bağlaçtan bir önceki ve sonraki kelime bir entity değil ise:\n",
    "                text = sent[old_conjunction.i+1:conjunction.i].text.strip() # eski bağlaçtan şu anki bağlaça kadar olan cümle metinden alınır\n",
    "                split_sentences.append(text) # alınan metin diziye eklenir\n",
    "                if add_old_entity == True: # eski entity nin eklenmesi gerekiyor ise:\n",
    "                    text = sent[old_entity.i].text.strip() # eski entity cümleden alınır\n",
    "                    text2 = sent[old_conjunction.i+2:conjunction.i].text.strip() # eski bağlaçtan ve entityden sonra şu anki bağlaça kadar olan cümle metinden alınır\n",
    "                    split_sentences.append(text + \" \" + text2) # eski entity ile alınan cümle arasına bir boşluk eklenerek diziye eklenir\n",
    "                    add_old_entity = False # eski entity alındığı için bool False değerine getirilir\n",
    "              old_conjunction = conjunction # eski bağlaç bir değişkende tutulur\n",
    "              k += 1 # baglac sayacı bir arttırılır\n",
    "          split_sentences.append(sent[conjunction.i + 1:].text.strip()) # son baglactan sonra kalan metin alınır\n",
    "          \n",
    "    while(\"\" in split_sentences): # son alınan metin boş string ise diziden silinir\n",
    "      split_sentences.remove(\"\")\n",
    "\n",
    "    return split_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 3\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      "--------------------\n",
      "--------------------\n",
      "////////////////////\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' kick güzelken', 'Twitch  güzelken', ' whatsapp çalışmıyor']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitting(entity : list, sentence: str):\n",
    "\n",
    "    sentenceresults = list()\n",
    "\n",
    "    if sentence.endswith(\".\"):\n",
    "        sentence = sentence[0:-1]\n",
    "\n",
    "    \n",
    "    numberofentities=howmanyentity(sentence=sentence,entity=entity) #change this function to handle empty spaces in the end\n",
    "    print(f\"numberofentities of this function is : {numberofentities}\") \n",
    "    if ( numberofentities == 1): # if there is only 1 entity\n",
    "        sentenceresults.append(sentence)\n",
    "\n",
    "    elif(numberofentities >1 ):\n",
    "        numberoffiilimsi = len(find_fiilimsi(sentence=sentence))\n",
    "        print(f\"Number of fiilimsi is :{numberoffiilimsi}\")\n",
    "\n",
    "        if(numberoffiilimsi==1) :\n",
    "            print(\"Hamza\") \n",
    "            entitiypositions = []\n",
    "            for i in entity:\n",
    "                entitiypositions.append(sentence.index(i))\n",
    "\n",
    "            fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0])\n",
    "            print(\"Emin\")\n",
    "            whereisfiilimsi = all(index < fiilimsiposition for index in entitiypositions)\n",
    "            if whereisfiilimsi:\n",
    "            \n",
    "                \n",
    "                for i in entitiypositions:\n",
    "                    entitylen = len(sentence[i:].split()[0])\n",
    "                    print(f\" Empty len is {entitylen}\")\n",
    "                    print(f\"entitiyposition: {entitylen}, fiilimsipositions: {fiilimsiposition}\")\n",
    "                    if i == 0:\n",
    "                        temp_sentence = sentence[:entitylen] + \" \" + sentence[fiilimsiposition:]\n",
    "                    if i !=0:\n",
    "                        temp_sentence = sentence[i:i+entitylen]+ \" \" + sentence[fiilimsiposition:]\n",
    "                    sentenceresults.append(temp_sentence)\n",
    "                    \n",
    "\n",
    "            else : # that means there is a fiilimsi between entities. there can be entity fiilimsi entity , entity entity fiilimsi entity\n",
    "                beforeentitiesposition = [] # BEFORE THE FİİLİMSİ ENTİTİES\n",
    "                afterentitiesposition = [] # AFTER THE FİİLİMSİ ENTİTİES\n",
    "                fiilimsiposition = sentence.find(find_fiilimsi(sentence=sentence)[0]) # FİİLİMSİ POİZİTON \n",
    "\n",
    "                for i in entity:\n",
    "                    if sentence.find(i) < fiilimsiposition:\n",
    "                        beforeentitiesposition.append(sentence.find(i))\n",
    "                    elif sentence.find(i) > fiilimsiposition:\n",
    "                        afterentitiesposition.append(sentence.find(i))\n",
    "                \n",
    "                #handle before entities\n",
    "                #handle after entities\n",
    "                #append the result\n",
    "                for positions in beforeentitiesposition: # [\"Twitch kick güzelken whatsapp çalışmıyor\"] [\"Twitch: 0 kick : 7 whatsapp :20\"][\"güzelken=12\"]\n",
    "                    print(\"--------------------\")\n",
    "                    \n",
    "                    entitylen = len(sentence[fiilimsiposition:].split()[0]) # güzelken = 12 güzelken len = 8\n",
    "                    subsentence = sentence[:fiilimsiposition+entitylen]   # sentence[:20] whatsapp dan öncesi\n",
    "                    tempb = subsentence\n",
    "                    for words in entity: #sentence is Twitch kick güzelken beforeentitiesposition is [twitch pozisyonu kick pozisyonu]\n",
    "                        if subsentence.find(words) == positions and len(beforeentitiesposition)!=1 : #  0 ve 7 eğer 20 eğer eşit değilse  0 7 20 yani 20 alıcak\n",
    "                            tempb=tempb.replace(words,\"\") # entity list contains Twitch kick turkiye. \n",
    "                    sentenceresults.append(tempb)\n",
    "                    \n",
    "                \n",
    "\n",
    "                for positions in afterentitiesposition: \n",
    "                    print(\"////////////////////\")\n",
    "                   \n",
    "                    entitylen = len(sentence[fiilimsiposition:].split()[0])\n",
    "                    subsentence = sentence[fiilimsiposition+entitylen:]\n",
    "                    tempa = subsentence\n",
    "                    for words in entity: # küçük bir hata var\n",
    "                        if subsentence.find(words) == positions : # with last part after the and  if the entity  is less than the position\n",
    "                            tempa = tempa.replace(words,\"\")\n",
    "                    sentenceresults.append(tempa)\n",
    "                    \n",
    "        \n",
    "\n",
    "    return sentenceresults\n",
    "\n",
    "#splitting([\"Vodafone\",\"Turkcell\"],\"Vodafone Turkcell güzelken çok pahalılar\") # that works\n",
    "entities3= [\"Vodafone\",\"Turkcell\"]\n",
    "example3 = \"Vodafone Turkcell güzelken çok pahalılar\"  # correct  # \n",
    "example16 = \"Vodafone güzelken Turkcell çok yavaş kalması can sıkıcı\"\n",
    "example5 = \"Twitch kick güzelken whatsapp çalışmıyor\" # correct \n",
    "\n",
    "ent5 = [\"Twitch\",\"kick\",\"whatsapp\"]\n",
    "\n",
    "splitting(entity=ent5,sentence= example5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vodafone', 'RedTarife']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_entities(entities,sentence):\n",
    "    entit = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in entities:\n",
    "            entit.append(word)\n",
    "    return entit\n",
    "\n",
    "sentence1 =\"Vodafone RedTarife güzelken çok pahalılar\"\n",
    "entity = [\"Vodafone\",\"Turkcell\",\"RedTarife\"]\n",
    "find_entities(entity,sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dependency_Parser(entities:list, text:str):\n",
    "    result = get_split_sentences_with_conjunction(entities, text)\n",
    "    sentence_results = []\n",
    "\n",
    "    for sentence in result:\n",
    "        entity_list = find_entities(entities,sentence)\n",
    "        sentence_results.append(splitting(entity_list,sentence))\n",
    "    return sentence_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numberofentities of this function is : 2\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      "--------------------\n",
      "////////////////////\n",
      "numberofentities of this function is : 2\n",
      "Number of fiilimsi is :1\n",
      "Hamza\n",
      "Emin\n",
      " Empty len is 8\n",
      "entitiyposition: 8, fiilimsipositions: 19\n",
      " Empty len is 9\n",
      "entitiyposition: 9, fiilimsipositions: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Vodafone güzelken', ' Turkcell parlak değil'],\n",
       " ['Vodafone güzelken çok pahalılar', 'RedTarife güzelken çok pahalılar']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Vodafone güzelken Turkcell parlak değil ve Vodafone RedTarife güzelken çok pahalılar \"\n",
    "sentence1 =\"Vodafone RedTarife güzelken çok pahalılar\"\n",
    "entity = [\"Vodafone\",\"Turkcell\",\"RedTarife\"]\n",
    "Dependency_Parser(entity,sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
