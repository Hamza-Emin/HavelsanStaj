{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base = 'https://www.sikayetvar.com/vodafone'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def extract_complaint_data(complaint_url):\n",
    "  \n",
    "    response = requests.get(complaint_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the title\n",
    "        title_tag = soup.find('h2', class_='complaint-detail-title')\n",
    "        title = title_tag.text.strip() if title_tag else \"\"\n",
    "\n",
    "        # Extract the explanation\n",
    "        explanation_tag = soup.find('div', class_='complaint-detail-description')\n",
    "        explanation = explanation_tag.text.strip() if explanation_tag else \"\"\n",
    "\n",
    "        return title, explanation\n",
    "    else:\n",
    "        print(f'Failed to retrieve complaint page. Status code: {response.status_code}')\n",
    "        return \"\", \"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to collect complaint links and extract data.\"\"\"\n",
    "    with open('vodafone_dataset.csv', mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Title', 'Explanation', 'Target', 'Link'])\n",
    "\n",
    "        for i in range(1, 51):\n",
    "            if i == 1:\n",
    "                base_url = base\n",
    "            else:\n",
    "                base_url = f\"{base}?page={i}\"\n",
    "                \n",
    "            response = requests.get(base_url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                articles = soup.find_all('article')\n",
    "\n",
    "                for article in articles:\n",
    "                    title_tag = article.find('h2', class_='complaint-title')\n",
    "                    if title_tag:\n",
    "                        # Extract and construct the full URL for the complaint\n",
    "                        complaint_link = title_tag.find('a')['href']\n",
    "                        complaint_url = f\"https://www.sikayetvar.com{complaint_link}\"\n",
    "\n",
    "                        # Extract data from each complaint page\n",
    "                        title, explanation = extract_complaint_data(complaint_url)\n",
    "                        writer.writerow([title, explanation, 0, complaint_url])\n",
    "\n",
    "                        # Print the result to the console (optional)\n",
    "                        print(f'Title: {title}')\n",
    "                        print(f'Explanation: {explanation}')\n",
    "                        print('---')\n",
    "\n",
    "                        time.sleep(1)\n",
    "            else:\n",
    "                print(f'Failed to retrieve main page. Status code: {response.status_code}')\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
